# -*- coding: utf-8 -*-
"""trendlens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dr4g017qPCxnNn2IV8YvB7-Gq03KViRE
"""

#pip install praw

import praw #Python Reddit API Wrapper
import pandas as pd
from datetime import datetime

reddit = praw.Reddit(client_id='lG4XBdXQZAoNiIXezvgwLg',
                     client_secret='5GL-uFnZ0EaIZ0AUb8Jli4IREBTmtg',
                     user_agent='trendlens-test-v1',
                     username='Available_News_2450',
                     password='Minhhoa12@')

#Pick the subreddits
subreddits = ['GenZ', 'AskReddit', 'funny', 'worldnews','amitheasshole']
limit = 1000 #number of posts per subreddit

posts_data = []

for sub in subreddits:
    subreddit = reddit.subreddit(sub)
    for post in subreddit.hot(limit=limit):
        posts_data.append({
            'subreddit': sub,
             'title': post.title,
            'score': post.score,
            'created': datetime.fromtimestamp(post.created_utc),
            'num_comments': post.num_comments,
            'selftext': post.selftext
            })

#Create a Dataframe
df_reddit = pd.DataFrame(posts_data)
df_reddit.head()

df_reddit.shape

df_reddit.to_csv('reddit_trending_posts.csv', index=False)

import pandas as pd
import requests
from io import BytesIO

# Load the dataset and handle potential encoding issues
# Define the URL for the dataset (hosted on GitHub)
url = 'https://raw.githubusercontent.com/jtmtran/reddit_trending_realtime/79508708ec9e2418136b61decce3a7f94c9345b3/reddit_trending_posts.csv'

# Fetch the dataset from the URL
response = requests.get(url)

if response.status_code == 200:
    try:
        # Use 'on_bad_lines' to handle problematic rows
        df_reddit = pd.read_csv(BytesIO(response.content), encoding='ISO-8859-1', on_bad_lines='skip')
        print("Dataset successfully loaded.")
        print(df_reddit.head())
    except Exception as e:
        print(f"Error loading dataset: {e}")
else:
    print(f"Failed to retrieve the file. Status code: {response.status_code}")

df_reddit.head()

"""##Clean Text + Extract Trending Words

###Clean Text
"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
nltk.download('wordnet')

#Combine title and selftext to capture the full thought, not just a piece of it.
df_reddit['text'] = df_reddit['title'].fillna('') + ' ' + df_reddit['selftext'].fillna('')

df_reddit

#Cleaner
def clean_text(text):
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text) #keep letters and numbers
    # Convert to lowercase
    text = text.lower()
    tokens = text.split() #split into individual words
    tokens = [word for word in tokens if word not in stop_words and len(word)>2]
    return ' '.join(tokens)

df_reddit['clean_text'] = df_reddit['text'].fillna('').apply(clean_text)

"""##Get Top Trending Words"""

def get_urban_definitions(term, max_defs=3):
    try:
        url = f"https://www.urbandictionary.com/define.php?term={term.replace(' ', '%20')}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        meaning_divs = soup.find_all('div', class_='meaning')
        if meaning_divs:
            return [d.text.strip() for d in meaning_divs[:max_defs]]
        else:
            return None
    except Exception as e:
        print(f"Error for term '{term}': {e}")
        return None

slang_phrases = [
    'could of', 'would of', 'no cap', 'rizz', 'delulu',
    'main character', 'goofy ahh', 'caught in 4k', 'sigma male',
    'npc', 'mid', 'based', 'ratio', 'skibidi', 'slay',
    'girl math', 'be so for real', 'it‚Äôs giving', 'yeet',
    "lit", "dope", "af", "sus"
]

from collections import Counter

phrase_counts = Counter()

for phrase in slang_phrases:
    count = df_reddit['clean_text'].str.contains(phrase, case=False, na=False).sum()
    phrase_counts[phrase] = count

df_slang = pd.DataFrame(phrase_counts.items(), columns=['term', 'frequency'])
df_slang = df_slang[df_slang['frequency'] > 0].sort_values(by='frequency', ascending=False).reset_index(drop=True)

df_slang['urban_definition'] = df_slang['term'].apply(get_urban_definitions)

def format_definitions(defs):
    if isinstance(defs, list):
        return '‚Äî ' + '\n‚Äî '.join(defs)
    return None

df_slang['definition_display'] = df_slang['urban_definition'].apply(format_definitions)

df_slang['urban_definition'].apply(type).value_counts()

df_slang

def clean_text(text):
    return text.replace('\r', '').replace('\n', ' ').strip()

df_slang['urban_definition'] = df_slang['urban_definition'].apply(
    lambda defs: [clean_text(d) for d in defs] if isinstance(defs, list) else None
)

#Test it on 1 word
get_urban_definitions("rizz")

#Test it on 1 word
get_urban_definitions("sus")

df_slang.to_csv('slang_terms.csv', index=False)

"""##Look Up Terms In Urban Dictionary

üëë Optional Next:
	‚Ä¢	Highlight words only trending in r/teenagers vs. other subs
	‚Ä¢	Add sentiment analysis
	‚Ä¢	Build the Streamlit dashboard
	‚Ä¢	Add a button to ‚Äúexplain this slang‚Äù

  Let me know what direction you‚Äôre vibing with next:
üîç deep dive into slang trends?
üìä visualize it?
üß† auto-detect if something‚Äôs ‚ÄúTikTok slang‚Äù?
"""



"""##Visualizations

###Top Slang by Frequency
"""

#df = pd.read_csv("https://github.com/jtmtran/reddit_trending_realtime/raw/refs/heads/main/slang_terms.csv")

'''# Load the dataset and handle potential encoding issues
# Define the URL for the dataset (hosted on GitHub)
url = 'https://github.com/jtmtran/reddit_trending_realtime/raw/refs/heads/main/slang_terms.csv'

# Fetch the dataset from the URL
response = requests.get(url)

if response.status_code == 200:
    try:
        # Use 'on_bad_lines' to handle problematic rows
        df_slangs = pd.read_csv(BytesIO(response.content), encoding='ISO-8859-1', on_bad_lines='skip')
        print("Dataset successfully loaded.")
        print(df_reddit.head())
    except Exception as e:
        print(f"Error loading dataset: {e}")
else:
    print(f"Failed to retrieve the file. Status code: {response.status_code}")
    '''

import matplotlib.pyplot as plt

top_n = 15
plt.figure(figsize=(10, 6))
plt.barh(df_slang['term'].head(top_n)[::-1], df_slang['frequency'].head(top_n)[::-1])
plt.title('Top Slang Terms from Reddit')
plt.xlabel('Frequency')
plt.ylabel('Slang Term')
plt.tight_layout()
plt.show()

"""###Word Cloud"""

from wordcloud import WordCloud

text = ' '.join(df_slang['term'].values)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Slang Word Cloud')
plt.show()

#pip install streamlit

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load default dataset
df = pd.read_csv("https://github.com/jtmtran/reddit_trending_realtime/raw/refs/heads/main/slang_terms.csv")

# Optional: Upload your own CSV
df_upload = st.file_uploader("Upload a new slang CSV file", type="csv")
if df_upload:
    df = pd.read_csv(df_upload)
    st.success("Uploaded new dataset!")

# Filter settings
st.sidebar.title("Filter Options")
min_freq = st.sidebar.slider("Minimum frequency", 1, int(df['frequency'].max()), 5)
top_n = st.sidebar.slider("Top N slang terms to display", 5, 50, 15)

# Apply filtering AFTER upload
filtered_df = df[df['frequency'] >= min_freq].sort_values(by='frequency', ascending=False).head(top_n)

# Display title
st.title("üó£Ô∏è Reddit Slang Trend Dashboard")
st.markdown("See what's trending, what it means, and how often it's used.")

# Bar chart
st.subheader("Top Slang Terms by Frequency")
fig, ax = plt.subplots(figsize=(10, 6))
ax.barh(filtered_df['term'][::-1], filtered_df['frequency'][::-1])
ax.set_xlabel("Frequency")
ax.set_ylabel("Term")
st.pyplot(fig)

# Word cloud
st.subheader("Slang Word Cloud")
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(filtered_df['term']))
st.image(wordcloud.to_array(), use_column_width=True)

# Definitions
st.subheader("Definitions")
selected_term = st.selectbox("Select a slang term to see its meaning:", filtered_df['term'])
def_row = df[df['term'] == selected_term]

if not def_row.empty:
    definition = def_row['definition_display'].values[0]

    if isinstance(definition, str) and definition.strip().lower() != "nan" and definition.strip() != "":
        defs = definition.split('\n')
        for d in defs:
            st.markdown(f"‚Ä¢ {d.strip()}")
    else:
        st.warning("No definition available for this term.")

#pip install streamlit wordcloud matplotlib