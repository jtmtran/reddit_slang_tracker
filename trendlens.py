# -*- coding: utf-8 -*-
"""trendlens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dr4g017qPCxnNn2IV8YvB7-Gq03KViRE
"""

import sys
IN_STREAMLIT = "streamlit" in sys.argv[0]

try:
    if not IN_STREAMLIT:
        from IPython.display import Markdown, display

        def md(text):
            display(Markdown(text))
    else:
        def md(text):
            pass
except ModuleNotFoundError:
    def md(text):
        pass

#pip install praw

#pip install beautifulsoup4

from bs4 import BeautifulSoup

import praw #Python Reddit API Wrapper
import pandas as pd
from datetime import datetime

reddit = praw.Reddit(client_id='lG4XBdXQZAoNiIXezvgwLg',
                     client_secret='5GL-uFnZ0EaIZ0AUb8Jli4IREBTmtg',
                     user_agent='trendlens-test-v1',
                     username='Available_News_2450',
                     password='Minhhoa12@')

#Pick the subreddits
subreddits = ['GenZ', 'AskReddit', 'funny', 'worldnews','amitheasshole']
limit = 1000 #number of posts per subreddit

posts_data = []

for sub in subreddits:
    subreddit = reddit.subreddit(sub)
    for post in subreddit.hot(limit=limit):
        posts_data.append({
            'subreddit': sub,
             'title': post.title,
            'score': post.score,
            'created': datetime.fromtimestamp(post.created_utc),
            'num_comments': post.num_comments,
            'selftext': post.selftext
            })

#Create a Dataframe
df_reddit = pd.DataFrame(posts_data)

if not IN_STREAMLIT:
    print(df_reddit.head())

if not IN_STREAMLIT:
    print(df_reddit.shape)

df_reddit.to_csv('reddit_trending_posts.csv', index=False)

import pandas as pd
import requests
from io import BytesIO

# Load the dataset and handle potential encoding issues
# Define the URL for the dataset (hosted on GitHub)
url = 'https://raw.githubusercontent.com/jtmtran/reddit_slang_tracker/685c4c7daaf64f8b3c810304fd95044c1beaace6/reddit_trending_posts.csv'

# Fetch the dataset from the URL
response = requests.get(url)

if response.status_code == 200:
    try:
        # Use 'on_bad_lines' to handle problematic rows
        df_reddit = pd.read_csv(BytesIO(response.content), encoding='ISO-8859-1', on_bad_lines='skip')
        print("Dataset successfully loaded.")
        print(df_reddit.head())
    except Exception as e:
        print(f"Error loading dataset: {e}")
else:
    print(f"Failed to retrieve the file. Status code: {response.status_code}")

if not IN_STREAMLIT:
    print(df_reddit.head())

if not IN_STREAMLIT:
    md("## Clean Text + Extract Trending Words")
    md("### Clean Text")

if not IN_STREAMLIT:
    print(df_reddit.shape)

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
nltk.download('wordnet')

#Combine title and selftext to capture the full thought, not just a piece of it.
df_reddit['text'] = df_reddit['title'].fillna('') + ' ' + df_reddit['selftext'].fillna('')

if not IN_STREAMLIT:
    print(df_reddit.head())

#Cleaner
def clean_text(text):
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text) #keep letters and numbers
    # Convert to lowercase
    text = text.lower()
    tokens = text.split() #split into individual words
    tokens = [word for word in tokens if word not in stop_words and len(word)>2]
    return ' '.join(tokens)

df_reddit['clean_text'] = df_reddit['text'].fillna('').apply(clean_text)

if not IN_STREAMLIT:
    md("##Get Top Trending Words")

def get_urban_definitions(term, max_defs=3):
    try:
        url = f"https://www.urbandictionary.com/define.php?term={term.replace(' ', '%20')}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        meaning_divs = soup.find_all('div', class_='meaning')
        if meaning_divs:
            return [d.text.strip() for d in meaning_divs[:max_defs]]
        else:
            return None
    except Exception as e:
        print(f"Error for term '{term}': {e}")
        return None

slang_phrases = [
    'could of', 'would of', 'no cap', 'rizz', 'delulu',
    'main character', 'goofy ahh', 'caught in 4k', 'sigma male',
    'npc', 'mid', 'based', 'ratio', 'skibidi', 'slay',
    'girl math', 'be so for real', 'it‚Äôs giving', 'yeet',
    "lit", "dope", "af", "sus"
]

if not IN_STREAMLIT:
    print(df_reddit.columns)

df_reddit.head()

from collections import defaultdict
import pandas as pd

# Create a nested dictionary to count slang usage per day
phrase_date_counts = defaultdict(int)

for phrase in slang_phrases:
    # Find where the phrase appears in Reddit posts
    mask = df_reddit['clean_text'].str.contains(phrase, case=False, na=False)
    df_reddit['created'] = pd.to_datetime(df_reddit['created'], errors='coerce')
    # Extract the 'created' date (strip time)
    matched_dates = df_reddit.loc[mask, 'created'].dt.date

    # Count how often each term appears per day
    for date in matched_dates:
        phrase_date_counts[(phrase, date)] += 1

# Convert to DataFrame
df_slang = pd.DataFrame(
    [(term, date, count) for (term, date), count in phrase_date_counts.items()],
    columns=['term', 'created_date', 'frequency']
)

# Optional: Sort by most used slang or latest date
df_slang = df_slang.sort_values(by=['created_date', 'frequency'], ascending=[True, False])

if not IN_STREAMLIT:
    md("##Look Up Terms In Urban Dictionary")

df_slang['urban_definition'] = df_slang['term'].apply(get_urban_definitions)

def format_definitions(defs):
    if isinstance(defs, list):
        return '‚Äî ' + '\n‚Äî '.join(defs)
    return None

df_slang['definition_display'] = df_slang['urban_definition'].apply(format_definitions)

df_slang['urban_definition'].apply(type).value_counts()

if not IN_STREAMLIT:
    print(df_slang.shape)

if not IN_STREAMLIT:
    print(df_slang.head())

def clean_text(text):
    return text.replace('\r', '').replace('\n', ' ').strip()

df_slang['urban_definition'] = df_slang['urban_definition'].apply(
    lambda defs: [clean_text(d) for d in defs] if isinstance(defs, list) else None
)

#Test it on 1 word
get_urban_definitions("rizz")

#Test it on 1 word
get_urban_definitions("sus")

df_slang.to_csv('slang_terms.csv', index=False)

if not IN_STREAMLIT:
    md("##Visualizations")
    md("###Top Slang by Frequency")

#df = pd.read_csv("https://github.com/jtmtran/reddit_trending_realtime/raw/refs/heads/main/slang_terms.csv")

import matplotlib.pyplot as plt

top_n = 15
plt.figure(figsize=(10, 6))
plt.barh(df_slang['term'].head(top_n)[::-1], df_slang['frequency'].head(top_n)[::-1])
plt.title('Top Slang Terms from Reddit')
plt.xlabel('Frequency')
plt.ylabel('Slang Term')
plt.tight_layout()
plt.show()

if not IN_STREAMLIT:
    md("###Word Cloud")

from wordcloud import WordCloud

text = ' '.join(df_slang['term'].values)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(12, 6))
#plt.imshow(wordcloud, interpolation='bilinear')
plt.imshow(wordcloud.to_array(), interpolation='bilinear')
plt.axis('off')
plt.title('Slang Word Cloud')
plt.show()

#pip install streamlit

import subprocess
import sys

# Only install in Colab
if 'google.colab' in sys.modules:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "altair"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "wordcloud"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "urbandict"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "streamlit"])

    import altair as alt
    alt.renderers.enable('default')

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load default dataset
df = pd.read_csv("https://raw.githubusercontent.com/jtmtran/reddit_slang_tracker/0872c84560999eff0b43a2fbae4b0f51e43cf92a/slang_terms.csv")
st.markdown(
    "<a href='https://github.com/jtmtran/reddit_trending_realtime' target='_blank'>"
    "<img style='position: absolute; top: 0; right: 0; border: 0;' "
    "src='https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png?resize=149%2C149' "
    "alt='Fork me on GitHub'>"
    "</a>",
    unsafe_allow_html=True
)


# Filter settings
st.sidebar.title("‚öôÔ∏è Controls")
st.sidebar.markdown("Filter how many terms to show or upload your own slang CSV.")
min_freq = st.sidebar.slider("Minimum frequency", 1, int(df['frequency'].max()), 5)
top_n = st.sidebar.slider("Top N slang terms to display", 5, 50, 15)
st.sidebar.caption("Adjust the sliders to explore more or less frequent slang.")

# Apply filtering AFTER upload
filtered_df = df[df['frequency'] >= min_freq].sort_values(by='frequency', ascending=False).head(top_n)

# Display title
st.title("üß† TrendLens: Real-Time Slang Radar")
st.markdown(
    "üëã Welcome to **TrendLens**, a live dashboard that scrapes Reddit posts to track trending slang.  \n"
    "üó£ Built for Gen Z, linguists, and trend-watchers.  \n"
    "üîç Explore, define, and visualize how slang moves through the internet ‚Äî in real time."
)

#!pip install urbandict
#from urbandict import define

if not IN_STREAMLIT:
    print(df.head())

col1, col2 = st.columns(2)
col1.metric("Unique Slang Terms", len(df['term'].unique()))
col2.metric("Most Frequent Term", df['term'].value_counts().idxmax())

# Bar chart
st.subheader("üìä Most Popular Slang Terms")
st.markdown("**üìà Summary:** This dashboard shows the top trending slang terms scraped from Reddit. Use the controls to explore frequency and meaning in real time.")

# Create a copy of df with total slang frequencies
df_top = df.groupby('term')['frequency'].sum().reset_index()

# Sort in ascending order for horizontal bar chart (highest at the top)
df_top = df_top.sort_values(by='frequency', ascending=True)

# Then plot
import matplotlib.pyplot as plt
plt.style.use('dark_background')

fig, ax = plt.subplots()
df_top.plot(kind='barh', x='term', y='frequency', ax=ax, color='skyblue', legend = False)

ax.set_xlabel('Frequency')
ax.set_ylabel('Term')
ax.set_title('Top Slang Terms (Most to Least)')

# Show chart based on environment
if 'google.colab' in sys.modules:
    plt.show()  # üëà show chart in Colab
else:
    st.pyplot(fig)  # üëà render chart in Streamlit

import altair as alt
import pandas as pd
import streamlit as st

# Ensure datetime format
df['created_date'] = pd.to_datetime(df['created_date'])

# Group data by date
daily_counts = df.groupby('created_date')['frequency'].sum().reset_index()

# Build the line chart
chart = alt.Chart(daily_counts).mark_line(point=True).encode(
    x=alt.X('created_date:T', title='Date'),
    y=alt.Y('frequency:Q', title='Mentions', scale=alt.Scale(domainMin=0)),
    tooltip=['created_date:T', 'frequency']
).properties(
    width=700,
    height=400,
    title='Trending Slang Over Time'
).configure_view(
    fill='black',
    stroke=None
).configure_axis(
    labelFontSize=12,
    titleFontSize=14,
    labelColor='white',
    titleColor='white'
).configure_title(
    fontSize=18,
    anchor='start',
    font='Helvetica',
    color='white'
).configure_legend(
    labelColor='white',
    titleColor='white'
).interactive()

if 'google.colab' in sys.modules:
    plt.show()  # üëà show chart in Colab
else:
    st.altair_chart(chart, use_container_width=True)  # üëà render chart in Streamlit

from wordcloud import WordCloud

st.subheader("‚òÅÔ∏è Visual Word Cloud of Slang")

df_wc = df.groupby('term')['frequency'].sum().reset_index()
df_wc = df_wc[df_wc['frequency'] >= min_freq].sort_values(by='frequency', ascending=False).head(top_n)

# Create a dictionary of terms and their frequency
word_freq = dict(zip(df_wc['term'], df_wc['frequency']))

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='black',
                      colormap='Pastel1',
                      prefer_horizontal=0.9
                      ).generate_from_frequencies(word_freq)
# Display using matplotlib for more control
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")  # Removes axes/legends
plt.tight_layout(pad=0)

st.pyplot(plt)
plt.show()

if st.checkbox("üîç Show Raw Reddit Posts"):
    st.dataframe(df_reddit[['subreddit', 'title', 'score']])

with st.sidebar.expander("‚ÑπÔ∏è About this project"):
    st.markdown("""
    **TrendLens** is a real-time slang tracking app powered by Reddit + Urban Dictionary.

    Created by [Jennie Tran](https://github.com/jtmtran)
    Built with: Python ‚Ä¢ Streamlit ‚Ä¢ NLP
    """)

st.subheader("üí¨ Try Your Own Slang (Manual Search)")
user_input = st.text_input("Type a slang word to look up its Urban Dictionary meaning:")

if user_input:
    try:
        ud_result = define(user_input)
        if ud_result:
            st.markdown(f"**Definition 1:**")
            st.info(ud_result[0]['def'])
            st.markdown(f"[üîó View on Urban Dictionary](https://www.urbandictionary.com/define.php?term={user_input})")
        else:
            st.warning("No definition found for this slang.")
    except Exception as e:
        st.error(f"Couldn't fetch definition. Try another word. Error: {e}")
else:
    st.markdown("üß™ Or select from trending Reddit slang below:")

st.subheader("üìñ Slang Definitions + Urban Dictionary")
selected_term = st.selectbox("Select a slang term to see its meaning:", filtered_df['term'])
def_row = df[df['term'] == selected_term]

if not def_row.empty:
  try:
    definition = def_row['definition_display'].values[0]
    if isinstance(definition, str) and definition.strip().lower() != "nan" and definition.strip():
        defs = definition.split('\n')
        for i, d in enumerate(defs, 1):
            st.markdown(f"**Definition {i}:**")
            st.info(d.strip())
        st.markdown(f"[üîó View on Urban Dictionary](https://www.urbandictionary.com/define.php?term={selected_term})")
    else:
        st.warning("No definition available for this term.")
  except Exception as e:
    st.error(f"Oops ‚Äî something went wrong loading the definition: {e}")

st.markdown("[![GitHub](https://img.shields.io/badge/GitHub-Repo-informational?style=flat&logo=github)](https://github.com/jtmtran/reddit_trending_realtime)")

#pip install streamlit wordcloud matplotlib

#Upload your own CSV
df_upload = st.file_uploader("Upload a new slang CSV file", type="csv")
if df_upload:
    df = pd.read_csv(df_upload)
    st.success("Uploaded new dataset!")

st.markdown("---")
st.caption("Built with ‚ù§Ô∏è by Jennie Tran | [GitHub](https://github.com/jtmtran)")