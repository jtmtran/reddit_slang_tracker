# -*- coding: utf-8 -*-
"""trendlens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dr4g017qPCxnNn2IV8YvB7-Gq03KViRE
"""

import sys
IN_STREAMLIT = "streamlit" in sys.argv[0]

try:
    if not IN_STREAMLIT:
        from IPython.display import Markdown, display

        def md(text):
            display(Markdown(text))
    else:
        def md(text):
            pass
except ModuleNotFoundError:
    def md(text):
        pass

#pip install praw

#pip install beautifulsoup4

from bs4 import BeautifulSoup

import praw #Python Reddit API Wrapper
import pandas as pd
from datetime import datetime

reddit = praw.Reddit(client_id='lG4XBdXQZAoNiIXezvgwLg',
                     client_secret='5GL-uFnZ0EaIZ0AUb8Jli4IREBTmtg',
                     user_agent='trendlens-test-v1',
                     username='Available_News_2450',
                     password='Minhhoa12@')

#Pick the subreddits
subreddits = ['GenZ', 'AskReddit', 'funny', 'worldnews','amitheasshole']
limit = 1000 #number of posts per subreddit

posts_data = []

for sub in subreddits:
    subreddit = reddit.subreddit(sub)
    for post in subreddit.hot(limit=limit):
        posts_data.append({
            'subreddit': sub,
             'title': post.title,
            'score': post.score,
            'created': datetime.fromtimestamp(post.created_utc),
            'num_comments': post.num_comments,
            'selftext': post.selftext
            })

#Create a Dataframe
df_reddit = pd.DataFrame(posts_data)

if not IN_STREAMLIT:
    print(df_reddit.head())

if not IN_STREAMLIT:
    print(df_reddit.shape)

df_reddit.to_csv('reddit_trending_posts.csv', index=False)

import pandas as pd
import requests
from io import BytesIO

# Load the dataset and handle potential encoding issues
# Define the URL for the dataset (hosted on GitHub)
url = 'https://raw.githubusercontent.com/jtmtran/reddit_slang_tracker/685c4c7daaf64f8b3c810304fd95044c1beaace6/reddit_trending_posts.csv'

# Fetch the dataset from the URL
response = requests.get(url)

if response.status_code == 200:
    try:
        # Use 'on_bad_lines' to handle problematic rows
        df_reddit = pd.read_csv(BytesIO(response.content), encoding='ISO-8859-1', on_bad_lines='skip')
        print("Dataset successfully loaded.")
        print(df_reddit.head())
    except Exception as e:
        print(f"Error loading dataset: {e}")
else:
    print(f"Failed to retrieve the file. Status code: {response.status_code}")

if not IN_STREAMLIT:
    print(df_reddit.head())

if not IN_STREAMLIT:
    md("## Clean Text + Extract Trending Words")
    md("### Clean Text")

if not IN_STREAMLIT:
    print(df_reddit.shape)

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
nltk.download('wordnet')

#Combine title and selftext to capture the full thought, not just a piece of it.
df_reddit['text'] = df_reddit['title'].fillna('') + ' ' + df_reddit['selftext'].fillna('')

if not IN_STREAMLIT:
    print(df_reddit.head())

#Cleaner
def clean_text(text):
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text) #keep letters and numbers
    # Convert to lowercase
    text = text.lower()
    tokens = text.split() #split into individual words
    tokens = [word for word in tokens if word not in stop_words and len(word)>2]
    return ' '.join(tokens)

df_reddit['clean_text'] = df_reddit['text'].fillna('').apply(clean_text)

if not IN_STREAMLIT:
    md("##Get Top Trending Words")

def get_urban_definitions(term, max_defs=3):
    try:
        url = f"https://www.urbandictionary.com/define.php?term={term.replace(' ', '%20')}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        meaning_divs = soup.find_all('div', class_='meaning')
        if meaning_divs:
            return [d.text.strip() for d in meaning_divs[:max_defs]]
        else:
            return None
    except Exception as e:
        print(f"Error for term '{term}': {e}")
        return None

slang_phrases = [
    'could of', 'would of', 'no cap', 'rizz', 'delulu',
    'main character', 'goofy ahh', 'caught in 4k', 'sigma male',
    'npc', 'mid', 'based', 'ratio', 'skibidi', 'slay',
    'girl math', 'be so for real', 'it‚Äôs giving', 'yeet',
    "lit", "dope", "af", "sus"
]

if not IN_STREAMLIT:
    print(df_reddit.columns)

df_reddit.head()

from collections import defaultdict
import pandas as pd

# Create a nested dictionary to count slang usage per day
phrase_date_counts = defaultdict(int)

for phrase in slang_phrases:
    # Find where the phrase appears in Reddit posts
    mask = df_reddit['clean_text'].str.contains(phrase, case=False, na=False)
    df_reddit['created'] = pd.to_datetime(df_reddit['created'], errors='coerce')
    # Extract the 'created' date (strip time)
    matched_dates = df_reddit.loc[mask, 'created'].dt.date

    # Count how often each term appears per day
    for date in matched_dates:
        phrase_date_counts[(phrase, date)] += 1

# Convert to DataFrame
df_slang = pd.DataFrame(
    [(term, date, count) for (term, date), count in phrase_date_counts.items()],
    columns=['term', 'created_date', 'frequency']
)

# Optional: Sort by most used slang or latest date
df_slang = df_slang.sort_values(by=['created_date', 'frequency'], ascending=[True, False])

if not IN_STREAMLIT:
    md("##Look Up Terms In Urban Dictionary")

df_slang['urban_definition'] = df_slang['term'].apply(get_urban_definitions)

def format_definitions(defs):
    if isinstance(defs, list):
        return '‚Äî ' + '\n‚Äî '.join(defs)
    return None

df_slang['definition_display'] = df_slang['urban_definition'].apply(format_definitions)

df_slang['urban_definition'].apply(type).value_counts()

if not IN_STREAMLIT:
    print(df_slang.shape)

if not IN_STREAMLIT:
    print(df_slang.head())

def clean_text(text):
    return text.replace('\r', '').replace('\n', ' ').strip()

df_slang['urban_definition'] = df_slang['urban_definition'].apply(
    lambda defs: [clean_text(d) for d in defs] if isinstance(defs, list) else None
)

#Test it on 1 word
get_urban_definitions("rizz")

#Test it on 1 word
get_urban_definitions("sus")

df_slang.to_csv('slang_terms.csv', index=False)

if not IN_STREAMLIT:
    md("##Visualizations")
    md("###Top Slang by Frequency")

#df = pd.read_csv("https://github.com/jtmtran/reddit_trending_realtime/raw/refs/heads/main/slang_terms.csv")

import matplotlib.pyplot as plt

top_n = 15
plt.figure(figsize=(10, 6))
plt.barh(df_slang['term'].head(top_n)[::-1], df_slang['frequency'].head(top_n)[::-1])
plt.title('Top Slang Terms from Reddit')
plt.xlabel('Frequency')
plt.ylabel('Slang Term')
plt.tight_layout()
plt.show()

if not IN_STREAMLIT:
    md("###Word Cloud")

from wordcloud import WordCloud

text = ' '.join(df_slang['term'].values)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(12, 6))
#plt.imshow(wordcloud, interpolation='bilinear')
plt.imshow(wordcloud.to_array(), interpolation='bilinear')
plt.axis('off')
plt.title('Slang Word Cloud')
plt.show()

#pip install streamlit

import subprocess
import sys

# Only install in Colab
if 'google.colab' in sys.modules:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "altair"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "wordcloud"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "urbandict"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "streamlit"])

    import altair as alt
    alt.renderers.enable('default')
    #from urbandict import define  # Safe to import here after install

#else:
    #from urbandict import define  # For environments like Streamlit where it's already installed

# Universal imports
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

#from urbandict import define
#print(define("sus"))

# Load default dataset
df = pd.read_csv("https://raw.githubusercontent.com/jtmtran/reddit_slang_tracker/0872c84560999eff0b43a2fbae4b0f51e43cf92a/slang_terms.csv")
st.markdown(
    "<a href='https://github.com/jtmtran/reddit_trending_realtime' target='_blank'>"
    "<img style='position: absolute; top: 0; right: 0; border: 0;' "
    "src='https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png?resize=149%2C149' "
    "alt='Fork me on GitHub'>"
    "</a>",
    unsafe_allow_html=True
)


# Filter settings
st.sidebar.title("‚öôÔ∏è Controls")
st.sidebar.markdown("Filter how many terms to show or upload your own slang CSV.")
min_freq = st.sidebar.slider("Minimum frequency", 1, int(df['frequency'].max()), 5)
top_n = st.sidebar.slider("Top N slang terms to display", 5, 50, 15)
st.sidebar.caption("Adjust the sliders to explore more or less frequent slang.")

# Apply filtering AFTER upload
filtered_df = df[df['frequency'] >= min_freq].sort_values(by='frequency', ascending=False).head(top_n)

# Display title
st.title("üß† TrendLens: Real-Time Slang Radar")
st.markdown(
    "üëã Welcome to **TrendLens**, a live dashboard that scrapes Reddit posts to track trending slang.  \n"
    "üó£ Built for Gen Z, linguists, and trend-watchers.  \n"
    "üîç Explore, define, and visualize how slang moves through the internet ‚Äî in real time."
)

if not IN_STREAMLIT:
    print(df.head())

with st.sidebar.expander("‚ÑπÔ∏è About this project"):
    st.markdown("""
    **TrendLens** is a real-time slang tracking app powered by Reddit + Urban Dictionary.

    It scrapes Reddit posts, detects emerging slang terms, and cross-references their meanings using the Urban Dictionary API.

    **Created by:** [Jennie Tran](https://github.com/jtmtran)
    **Built with:** Python ‚Ä¢ Streamlit ‚Ä¢ NLP

    [![GitHub](https://img.shields.io/badge/GitHub-Repo-informational?style=flat&logo=github)](https://github.com/jtmtran/reddit_trending_realtime)
    """)

col1, col2 = st.columns(2)
col1.metric("Unique Slang Terms", len(df['term'].unique()))
col2.metric("Most Frequent Term", df['term'].value_counts().idxmax())

tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    "üìà Trends",
    "‚òÅÔ∏è Word Cloud",
    "üìä Bar Chart",
    "üí¨ Manual Search",
    "üìñ Slang Lookup",
    "üìÇ Raw Data",
    "üìÆ Feedback"
])

import altair as alt
import pandas as pd
import streamlit as st

with tab1:
  # Ensure datetime format
  df['created_date'] = pd.to_datetime(df['created_date'])

  # Group data by date
  daily_counts = df.groupby('created_date')['frequency'].sum().reset_index()

  # Build the line chart
  chart = alt.Chart(daily_counts).mark_line(point=True).encode(
      x=alt.X('created_date:T', title='Date'),
      y=alt.Y('frequency:Q', title='Mentions', scale=alt.Scale(domainMin=0)),
      tooltip=['created_date:T', 'frequency']
  ).properties(
      width=700,
      height=400,
      title='Trending Slang Over Time'
  ).configure_view(
      fill='black',
      stroke=None
  ).configure_axis(
      labelFontSize=12,
      titleFontSize=14,
      labelColor='white',
      titleColor='white'
  ).configure_title(
      fontSize=18,
      anchor='start',
      font='Helvetica',
      color='white'
  ).configure_legend(
      labelColor='white',
      titleColor='white'
  ).interactive()

  if 'google.colab' in sys.modules:
      plt.show()  # üëà show chart in Colab
  else:
      st.altair_chart(chart, use_container_width=True)  # üëà render chart in Streamlit

  st.markdown("This timeline shows how often each slang term appeared in Reddit posts over time. Sudden spikes may reflect viral trends, memes, or events.")

with tab2:
  st.subheader("‚òÅÔ∏è Visual Word Cloud of Slang")

  df_wc = df.groupby('term')['frequency'].sum().reset_index()
  df_wc = df_wc[df_wc['frequency'] >= min_freq].sort_values(by='frequency', ascending=False).head(top_n)

  # Create a dictionary of terms and their frequency
  word_freq = dict(zip(df_wc['term'], df_wc['frequency']))

  # Generate word cloud
  wordcloud = WordCloud(width=800, height=400, background_color='black',
                        colormap='Pastel1',
                        prefer_horizontal=0.9
                        ).generate_from_frequencies(word_freq)
  # Display using matplotlib for more control
  plt.figure(figsize=(10, 5))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis("off")  # Removes axes/legends
  plt.tight_layout(pad=0)

  st.pyplot(plt)
  plt.show()

  st.markdown("The word cloud highlights the most frequently mentioned slang terms. Larger words represent more popular or commonly used expressions.")

with tab3:
    # Bar chart
    st.subheader("üìä Most Popular Slang Terms")
    st.markdown("**üìà Summary:** This dashboard shows the top trending slang terms scraped from Reddit. Use the controls to explore frequency and meaning in real time.")

    # Create a copy of df with total slang frequencies
    df_top = df.groupby('term')['frequency'].sum().reset_index()

    # Sort in descending order (highest at the top)
    df_top = df_top.sort_values(by='frequency', ascending=True).head(10)

    # Then plot
    import matplotlib.pyplot as plt
    plt.style.use('dark_background')

    fig, ax = plt.subplots()
    df_top.plot(kind='barh', x='term', y='frequency', ax=ax, color='skyblue', legend=False)

    ax.set_xlabel('Frequency')
    ax.set_ylabel('Term')
    ax.set_title('Top Slang Terms (Most to Least)')

    # Show chart based on environment
    if 'google.colab' in sys.modules:
        plt.show()
    else:
        st.pyplot(fig)

    st.markdown("This chart ranks the top 20 most used slang terms from Reddit. It helps identify which phrases dominate online conversations.")

import requests
with tab4:
  # Define your API call function
  def fetch_urban_definition(term):
      url = "https://mashape-community-urban-dictionary.p.rapidapi.com/define"
      headers = {
          "X-RapidAPI-Key": "083a1412b9msh0d9f5a60f7c9649p1e37a2jsn078a7118175c",
          "X-RapidAPI-Host": "mashape-community-urban-dictionary.p.rapidapi.com"
      }
      params = {"term": term}

      response = requests.get(url, headers=headers, params=params)
      data = response.json()

      if data.get("list"):
          return data["list"][0]["definition"]
      return None


  # Streamlit input section
  st.subheader("üí¨ Try Your Own Slang (Manual Search)")
  user_input = st.text_input("Type a slang word to look up its Urban Dictionary meaning:")

  if user_input:
      try:
          definition = fetch_urban_definition(user_input)

          if definition:
              st.markdown("**Definition 1:**")
              st.info(definition.strip())
              st.markdown(f"[üîó View on Urban Dictionary](https://www.urbandictionary.com/define.php?term={user_input})")
          else:
              st.warning(f"No definition found for '**{user_input}**' on Urban Dictionary.")
      except Exception as e:
          st.error(f"Something went wrong. Error: {e}")
  else:
      st.markdown("üß™ Or select from trending Reddit slang in the next tab")

  #st.markdown("Type in any slang word to look up its Urban Dictionary meaning. Use this to explore terms outside the current Reddit trends.")

with tab5:
  # Existing dropdown for filtered slang
  st.subheader("üìñ Slang Definitions + Urban Dictionary")
  selected_term = st.selectbox("Select a slang term to see its meaning:", filtered_df['term'])
  def_row = df[df['term'] == selected_term]

  if not def_row.empty:
      try:
          definition = def_row['definition_display'].values[0]
          if isinstance(definition, str) and definition.strip().lower() != "nan" and definition.strip():
              defs = definition.split('\n')
              for i, d in enumerate(defs, 1):
                  st.markdown(f"**Definition {i}:**")
                  st.info(d.strip())
              st.markdown(f"[üîó View on Urban Dictionary](https://www.urbandictionary.com/define.php?term={selected_term})")
          else:
              st.warning("No definition available for this term.")
      except Exception as e:
          st.error(f"Oops ‚Äî something went wrong loading the definition: {e}")

  st.markdown("Select from the trending terms found on Reddit to view their meanings. Definitions are sourced from Urban Dictionary or scraped content.")

with tab6:
  if st.checkbox("üîç Show Raw Reddit Posts"):
      st.markdown("**Note:** 'Score' refers to a Reddit post's popularity ‚Äî it's the number of upvotes minus downvotes.")
      st.dataframe(df_reddit[['subreddit', 'title', 'score']].sort_values(by='score', ascending=False))

  st.markdown("Here‚Äôs the raw Reddit post data used in the analysis. You can sort by subreddit or score to explore context and popularity.")

with tab7:
    with st.form("feedback_form"):
        st.write("üìÆ Have thoughts or suggestions?")
        feedback = st.text_area("Leave your feedback here:")
        submitted = st.form_submit_button("Submit")

        if submitted and feedback.strip():
            st.success("Thanks for your feedback! üôå")
            # Optional: save to file or Google Sheet

st.markdown("---")
st.caption("Built with ‚ù§Ô∏è by Jennie Tran | [GitHub](https://github.com/jtmtran)")