# -*- coding: utf-8 -*-
"""trendlens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dr4g017qPCxnNn2IV8YvB7-Gq03KViRE
"""

import sys
IN_STREAMLIT = "streamlit" in sys.argv[0]

try:
    if not IN_STREAMLIT:
        from IPython.display import Markdown, display

        def md(text):
            display(Markdown(text))
    else:
        def md(text):
            pass
except ModuleNotFoundError:
    def md(text):
        pass

#pip install praw

#pip install beautifulsoup4

from bs4 import BeautifulSoup

import praw #Python Reddit API Wrapper
import pandas as pd
from datetime import datetime

reddit = praw.Reddit(client_id='lG4XBdXQZAoNiIXezvgwLg',
                     client_secret='5GL-uFnZ0EaIZ0AUb8Jli4IREBTmtg',
                     user_agent='trendlens-test-v1',
                     username='Available_News_2450',
                     password='Minhhoa12@')

#Pick the subreddits
subreddits = ['GenZ', 'AskReddit', 'funny', 'worldnews','amitheasshole']
limit = 1000 #number of posts per subreddit

posts_data = []

for sub in subreddits:
    subreddit = reddit.subreddit(sub)
    for post in subreddit.hot(limit=limit):
        posts_data.append({
            'subreddit': sub,
             'title': post.title,
            'score': post.score,
            'created': datetime.fromtimestamp(post.created_utc),
            'num_comments': post.num_comments,
            'selftext': post.selftext
            })

#Create a Dataframe
df_reddit = pd.DataFrame(posts_data)

if not IN_STREAMLIT:
    print(df_reddit.head())

if not IN_STREAMLIT:
    print(df_reddit.shape)

df_reddit.to_csv('reddit_trending_posts.csv', index=False)

import pandas as pd
import requests
from io import BytesIO

# Load the dataset and handle potential encoding issues
# Define the URL for the dataset (hosted on GitHub)
url = 'https://raw.githubusercontent.com/jtmtran/reddit_slang_tracker/685c4c7daaf64f8b3c810304fd95044c1beaace6/reddit_trending_posts.csv'

# Fetch the dataset from the URL
response = requests.get(url)

if response.status_code == 200:
    try:
        # Use 'on_bad_lines' to handle problematic rows
        df_reddit = pd.read_csv(BytesIO(response.content), encoding='ISO-8859-1', on_bad_lines='skip')
        print("Dataset successfully loaded.")
        print(df_reddit.head())
    except Exception as e:
        print(f"Error loading dataset: {e}")
else:
    print(f"Failed to retrieve the file. Status code: {response.status_code}")

if not IN_STREAMLIT:
    print(df_reddit.head())

if not IN_STREAMLIT:
    md("## Clean Text + Extract Trending Words")
    md("### Clean Text")

if not IN_STREAMLIT:
    print(df_reddit.shape)

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
nltk.download('wordnet')

#Combine title and selftext to capture the full thought, not just a piece of it.
df_reddit['text'] = df_reddit['title'].fillna('') + ' ' + df_reddit['selftext'].fillna('')

if not IN_STREAMLIT:
    print(df_reddit.head())

#Cleaner
def clean_text(text):
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text) #keep letters and numbers
    # Convert to lowercase
    text = text.lower()
    tokens = text.split() #split into individual words
    tokens = [word for word in tokens if word not in stop_words and len(word)>2]
    return ' '.join(tokens)

df_reddit['clean_text'] = df_reddit['text'].fillna('').apply(clean_text)

if not IN_STREAMLIT:
    md("##Get Top Trending Words")

def get_urban_definitions(term, max_defs=3):
    try:
        url = f"https://www.urbandictionary.com/define.php?term={term.replace(' ', '%20')}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        meaning_divs = soup.find_all('div', class_='meaning')
        if meaning_divs:
            return [d.text.strip() for d in meaning_divs[:max_defs]]
        else:
            return None
    except Exception as e:
        print(f"Error for term '{term}': {e}")
        return None

slang_phrases = [
    'could of', 'would of', 'no cap', 'rizz', 'delulu',
    'main character', 'goofy ahh', 'caught in 4k', 'sigma male',
    'npc', 'mid', 'based', 'ratio', 'skibidi', 'slay',
    'girl math', 'be so for real', 'it‚Äôs giving', 'yeet',
    "lit", "dope", "af", "sus"
]

if not IN_STREAMLIT:
    print(df_reddit.columns)

df_reddit.head()

from collections import defaultdict
import pandas as pd

# Create a nested dictionary to count slang usage per day
phrase_date_counts = defaultdict(int)

for phrase in slang_phrases:
    # Find where the phrase appears in Reddit posts
    mask = df_reddit['clean_text'].str.contains(phrase, case=False, na=False)
    df_reddit['created'] = pd.to_datetime(df_reddit['created'], errors='coerce')
    # Extract the 'created' date (strip time)
    matched_dates = df_reddit.loc[mask, 'created'].dt.date

    # Count how often each term appears per day
    for date in matched_dates:
        phrase_date_counts[(phrase, date)] += 1

# Convert to DataFrame
df_slang = pd.DataFrame(
    [(term, date, count) for (term, date), count in phrase_date_counts.items()],
    columns=['term', 'created_date', 'frequency']
)

# Optional: Sort by most used slang or latest date
df_slang = df_slang.sort_values(by=['created_date', 'frequency'], ascending=[True, False])

if not IN_STREAMLIT:
    md("##Look Up Terms In Urban Dictionary")

df_slang['urban_definition'] = df_slang['term'].apply(get_urban_definitions)

def format_definitions(defs):
    if isinstance(defs, list):
        return '‚Äî ' + '\n‚Äî '.join(defs)
    return None

df_slang['definition_display'] = df_slang['urban_definition'].apply(format_definitions)

df_slang['urban_definition'].apply(type).value_counts()

if not IN_STREAMLIT:
    print(df_slang.shape)

if not IN_STREAMLIT:
    print(df_slang.head())

def clean_text(text):
    return text.replace('\r', '').replace('\n', ' ').strip()

df_slang['urban_definition'] = df_slang['urban_definition'].apply(
    lambda defs: [clean_text(d) for d in defs] if isinstance(defs, list) else None
)

#Test it on 1 word
get_urban_definitions("rizz")

#Test it on 1 word
get_urban_definitions("sus")

df_slang.to_csv('slang_terms.csv', index=False)

if not IN_STREAMLIT:
    md("##Visualizations")
    md("###Top Slang by Frequency")

#df = pd.read_csv("https://github.com/jtmtran/reddit_trending_realtime/raw/refs/heads/main/slang_terms.csv")

import matplotlib.pyplot as plt

top_n = 15
plt.figure(figsize=(10, 6))
plt.barh(df_slang['term'].head(top_n)[::-1], df_slang['frequency'].head(top_n)[::-1])
plt.title('Top Slang Terms from Reddit')
plt.xlabel('Frequency')
plt.ylabel('Slang Term')
plt.tight_layout()
plt.show()

if not IN_STREAMLIT:
    md("###Word Cloud")

from wordcloud import WordCloud

text = ' '.join(df_slang['term'].values)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(12, 6))
#plt.imshow(wordcloud, interpolation='bilinear')
plt.imshow(wordcloud.to_array(), interpolation='bilinear')
plt.axis('off')
plt.title('Slang Word Cloud')
plt.show()

#pip install streamlit

import subprocess
import sys

# Only install in Colab
if 'google.colab' in sys.modules:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "altair"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "wordcloud"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "urbandict"])
    subprocess.check_call([sys.executable, "-m", "pip", "install", "streamlit"])

    import altair as alt
    alt.renderers.enable('default')
    #from urbandict import define  # Safe to import here after install

#else:
    #from urbandict import define  # For environments like Streamlit where it's already installed

# Universal imports
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

#from urbandict import define
#print(define("sus"))

# Load default dataset
df = pd.read_csv("https://raw.githubusercontent.com/jtmtran/reddit_slang_tracker/0872c84560999eff0b43a2fbae4b0f51e43cf92a/slang_terms.csv")
st.markdown(
    "<a href='https://github.com/jtmtran/reddit_trending_realtime' target='_blank'>"
    "<img style='position: absolute; top: 0; right: 0; border: 0;' "
    "src='https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png?resize=149%2C149' "
    "alt='Fork me on GitHub'>"
    "</a>",
    unsafe_allow_html=True
)

# Filter settings
st.sidebar.title("‚öôÔ∏è Controls")
max_terms = df['term'].nunique()
st.sidebar.caption(f"Detected **{max_terms} unique slang terms**, used a total of **{int(df['frequency'].sum())} times** in Reddit posts.")
min_freq = st.sidebar.slider("Minimum frequency", 1, int(df['frequency'].max()), 5)
top_n = st.sidebar.slider("How many top slang terms to show?", 5, max_terms, min(10, max_terms))
st.sidebar.caption("Adjust the sliders to explore more or less frequent slang.")

# Apply filtering AFTER upload
filtered_df = df[df['frequency'] >= min_freq].sort_values(by='frequency', ascending=False).head(top_n)

# Display title
st.title("üß† TrendLens: Track What‚Äôs Poppin‚Äô")
st.markdown(
    "üëã Welcome to **TrendLens**, a live dashboard that scrapes Reddit posts to track trending slang.  \n"
    "üó£ Built for Gen Z, linguists, and trend-watchers.  \n"
    "üîç Explore, define, and visualize how slang moves through the internet ‚Äî in real time."
)

if not IN_STREAMLIT:
    print(df.head())

with st.sidebar.expander("‚ÑπÔ∏è About this project"):
    st.markdown("""
    **TrendLens** is a real-time slang tracking app powered by Reddit + Urban Dictionary.

    It scrapes Reddit posts, detects emerging slang terms, and cross-references their meanings using the Urban Dictionary API.

    **Created by:** [Jennie Tran](https://github.com/jtmtran)
    **Built with:** Python ‚Ä¢ Streamlit ‚Ä¢ NLP

    [![GitHub](https://img.shields.io/badge/GitHub-Repo-informational?style=flat&logo=github)](https://github.com/jtmtran/reddit_trending_realtime)
    """)

col1, col2 = st.columns(2)
col1.metric("Unique Slang Terms", len(df['term'].unique()))
col2.metric("Most Frequent Term", df['term'].value_counts().idxmax())

tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    "üìà Trends",
    "‚òÅÔ∏è Word Cloud",
    "üìä Bar Chart",
    "üí¨ Manual Search",
    "üìñ Slang Lookup",
    "üìÇ Raw Data",
    "üìÆ Feedback"
])

import altair as alt

with tab1:
    st.subheader("Trending Slang Over Time")
    st.markdown("**üìà Summary:**")
    st.markdown("This timeline shows how often each slang term appeared in Reddit posts over time. Sudden spikes may reflect viral trends, memes, or events.")

    # Ensure datetime format
    df['created_date'] = pd.to_datetime(df['created_date'])

    # Group data by date
    daily_counts = df.groupby('created_date')['frequency'].sum().reset_index()

    # Get max and min stats
    max_day = daily_counts.loc[daily_counts['frequency'].idxmax()]
    min_day = daily_counts.loc[daily_counts['frequency'].idxmin()]

    # Main chart
    base_chart = alt.Chart(daily_counts).mark_line(point=True).encode(
        x=alt.X('created_date:T', title='Date'),
        y=alt.Y('frequency:Q', title='Mentions', scale=alt.Scale(domainMin=0)),
        tooltip=['created_date:T', 'frequency']
    ).configure_view(
        fill='black',
        stroke=None
    ).configure_axis(
        labelFontSize=12,
        titleFontSize=14,
        labelColor='white',
        titleColor='white'
    ).configure_title(
        fontSize=18,
        anchor='start',
        font='Helvetica',
        color='white'
    ).configure_legend(
        labelColor='white',
        titleColor='white'
    ).interactive()

    # Highlight max point
    highlight_df = pd.DataFrame({
        "created_date": [max_day["created_date"]],
        "frequency": [max_day["frequency"]]
    })

    highlight = alt.Chart(highlight_df).mark_point(
        color='red', size=100
    ).encode(
        x='created_date:T',
        y='frequency:Q'
    )

    # Add label above the red point
    text_label = alt.Chart(highlight_df).mark_text(
        align='left', dx=5, dy=-10, color='white', fontSize=14
    ).encode(
        x='created_date:T',
        y='frequency:Q',
        text='frequency:Q'  # ‚Üê Use column name, not alt.value()
    )

    # Combine and show chart
    combined_chart = base_chart + highlight + text_label
    st.altair_chart(combined_chart, use_container_width=True)

    # Display insights
    st.metric(
        label="üìå Most Active Day",
        value=max_day['created_date'].strftime("%b %d, %Y"),
        delta=f"{max_day['frequency']} mentions"
    )

    st.markdown(f"üîª **Lowest Mentions:** {min_day['frequency']} on {min_day['created_date'].strftime('%b %d, %Y')}")

with tab2:
    st.subheader("Visual Word Cloud of Slang")
    st.markdown("**üìà Summary:**")
    st.markdown(
        "The word cloud highlights the most frequently mentioned slang terms. "
        "Larger words represent more popular or commonly used expressions.")
    df_wc = df.groupby('term')['frequency'].sum().reset_index()
    df_wc = df_wc[df_wc['frequency'] >= min_freq].sort_values(by='frequency', ascending=False).head(top_n)

    word_freq = dict(zip(df_wc['term'], df_wc['frequency']))

    wordcloud = WordCloud(
        width=800, height=400, background_color='black',
        colormap='Pastel1', prefer_horizontal=0.9
    ).generate_from_frequencies(word_freq)

    st.image(wordcloud.to_array(), use_container_width=True, channels="RGB", output_format="PNG")

with tab3:
    # Bar chart
    st.subheader("Most Popular Slang Terms")
    st.markdown("**üìà Summary:**")
    st.markdown("This chart ranks the top 10 most used slang terms from Reddit. It helps identify which phrases dominate online conversations. Use the controls to explore frequency and meaning in real time.")

    # Create a copy of df with total slang frequencies
    df_top = df.groupby('term')['frequency'].sum().reset_index()

    # Sort in descending order (highest at the top)
    df_top = df_top.sort_values(by='frequency', ascending=True).head(10)

    # Then plot
    import matplotlib.pyplot as plt
    plt.style.use('dark_background')

    fig, ax = plt.subplots()
    df_top.plot(kind='barh', x='term', y='frequency', ax=ax, color='skyblue', legend=False)

    ax.set_xlabel('Frequency')
    ax.set_ylabel('Term')
    ax.set_title('Top Slang Terms (Most to Least)')

    # Show chart based on environment
    if 'google.colab' in sys.modules:
        plt.show()
    else:
        st.pyplot(fig)

import requests

with tab4:
    st.subheader("Try Your Own Slang (Manual Search)")
    user_input = st.text_input("Type a slang word to look up its Urban Dictionary meaning:")

    if user_input:
        try:
            # Call API directly here instead of in a helper function
            url = "https://mashape-community-urban-dictionary.p.rapidapi.com/define"
            headers = {
                "X-RapidAPI-Key": "083a1412b9msh0d9f5a60f7c9649p1e37a2jsn078a7118175c",
                "X-RapidAPI-Host": "mashape-community-urban-dictionary.p.rapidapi.com"
            }
            response = requests.get(url, headers=headers, params={"term": user_input})
            data = response.json()

            if data.get("list"):
                st.markdown(f"**Definitions for _{user_input}_:**")
                for i, entry in enumerate(data["list"][:3], 1):  # Show top 3 definitions
                    def_text = entry.get("definition", "").strip()
                    if def_text:
                        st.markdown(f"**Definition {i}:**")
                        st.info(def_text)
                st.markdown(f"[üîó View more on Urban Dictionary](https://www.urbandictionary.com/define.php?term={user_input})")
            else:
                st.warning(f"No definition found for '**{user_input}**' on Urban Dictionary.")
        except Exception as e:
            st.error(f"Something went wrong. Error: {e}")
    else:
        st.markdown("üß™ Or select from trending Reddit slang in the next tab")

with tab5:
  # Existing dropdown for filtered slang
  st.subheader("Slang Definitions + Urban Dictionary")
  #selected_term = st.selectbox("Select a slang term to see its meaning:", filtered_df['term'])
  all_terms = sorted(df['term'].unique())
  selected_term = st.selectbox("Select a slang term to see its meaning:", all_terms)
  def_row = df[df['term'] == selected_term]

  if not def_row.empty:
      try:
          definition = def_row['definition_display'].values[0]
          if isinstance(definition, str) and definition.strip().lower() != "nan" and definition.strip():
              defs = definition.split('\n')
              for i, d in enumerate(defs, 1):
                  st.markdown(f"**Definition {i}:**")
                  st.info(d.strip())
              st.markdown(f"[üîó View on Urban Dictionary](https://www.urbandictionary.com/define.php?term={selected_term})")
          else:
              st.warning("No definition available for this term.")
      except Exception as e:
          st.error(f"Oops ‚Äî something went wrong loading the definition: {e}")

  st.markdown("Select from the trending terms found on Reddit to view their meanings. Definitions are sourced from Urban Dictionary or scraped content.")

with tab6:
  if st.checkbox("üîç Show Raw Reddit Posts"):
      st.markdown("**Note:** 'Score' refers to a Reddit post's popularity ‚Äî it's the number of upvotes minus downvotes.")
      st.dataframe(df_reddit[['subreddit', 'title', 'score']].sort_values(by='score', ascending=False))

  st.markdown("Here‚Äôs the raw Reddit post data used in the analysis. You can sort by subreddit or score to explore context and popularity.")

with tab7:
    with st.form("feedback_form"):
        st.write("Have thoughts or suggestions?")
        feedback = st.text_area("Leave your feedback here:")
        submitted = st.form_submit_button("Submit")

        if submitted and feedback.strip():
            st.success("Thanks for your feedback! üôå")
            # Optional: save to file or Google Sheet

st.markdown("---")
st.caption("Built with ‚ù§Ô∏è by Jennie Tran | [GitHub](https://github.com/jtmtran)")