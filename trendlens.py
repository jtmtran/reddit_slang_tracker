# -*- coding: utf-8 -*-
"""trendlens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dr4g017qPCxnNn2IV8YvB7-Gq03KViRE
"""

#pip install praw

import praw #Python Reddit API Wrapper
import pandas as pd
from datetime import datetime

reddit = praw.Reddit(client_id='lG4XBdXQZAoNiIXezvgwLg',
                     client_secret='5GL-uFnZ0EaIZ0AUb8Jli4IREBTmtg',
                     user_agent='trendlens-test-v1',
                     username='Available_News_2450',
                     password='Minhhoa12@')

#Pick the subreddits
subreddits = ['GenZ', 'AskReddit', 'funny', 'worldnews','amitheasshole']
limit = 1000 #number of posts per subreddit

posts_data = []

for sub in subreddits:
    subreddit = reddit.subreddit(sub)
    for post in subreddit.hot(limit=limit):
        posts_data.append({
            'subreddit': sub,
             'title': post.title,
            'score': post.score,
            'created': datetime.fromtimestamp(post.created_utc),
            'num_comments': post.num_comments,
            'selftext': post.selftext
            })

#Create a Dataframe
df_reddit = pd.DataFrame(posts_data)
df_reddit.head()

df_reddit.shape

df_reddit.to_csv('reddit_trending_posts.csv', index=False)

import pandas as pd
import requests
from io import BytesIO

# Load the dataset and handle potential encoding issues
# Define the URL for the dataset (hosted on GitHub)
url = 'https://raw.githubusercontent.com/jtmtran/reddit_trending_realtime/79508708ec9e2418136b61decce3a7f94c9345b3/reddit_trending_posts.csv'

# Fetch the dataset from the URL
response = requests.get(url)

if response.status_code == 200:
    try:
        # Use 'on_bad_lines' to handle problematic rows
        df_reddit = pd.read_csv(BytesIO(response.content), encoding='ISO-8859-1', on_bad_lines='skip')
        print("Dataset successfully loaded.")
        print(df_reddit.head())
    except Exception as e:
        print(f"Error loading dataset: {e}")
else:
    print(f"Failed to retrieve the file. Status code: {response.status_code}")

df_reddit.head()

"""##Clean Text + Extract Trending Words

###Clean Text
"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
nltk.download('wordnet')

#Combine title and selftext to capture the full thought, not just a piece of it.
df_reddit['text'] = df_reddit['title'].fillna('') + ' ' + df_reddit['selftext'].fillna('')

df_reddit

#Cleaner
def clean_text(text):
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text) #keep letters and numbers
    # Convert to lowercase
    text = text.lower()
    tokens = text.split() #split into individual words
    tokens = [word for word in tokens if word not in stop_words and len(word)>2]
    return ' '.join(tokens)

df_reddit['clean_text'] = df_reddit['text'].fillna('').apply(clean_text)

"""##Get Top Trending Words"""

def get_urban_definitions(term, max_defs=3):
    try:
        url = f"https://www.urbandictionary.com/define.php?term={term.replace(' ', '%20')}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        meaning_divs = soup.find_all('div', class_='meaning')
        if meaning_divs:
            return [d.text.strip() for d in meaning_divs[:max_defs]]
        else:
            return None
    except Exception as e:
        print(f"Error for term '{term}': {e}")
        return None

slang_phrases = [
    'could of', 'would of', 'no cap', 'rizz', 'delulu',
    'main character', 'goofy ahh', 'caught in 4k', 'sigma male',
    'npc', 'mid', 'based', 'ratio', 'skibidi', 'slay',
    'girl math', 'be so for real', 'it‚Äôs giving', 'yeet',
    "lit", "dope", "af", "sus"
]

from collections import Counter

phrase_counts = Counter()

for phrase in slang_phrases:
    count = df_reddit['clean_text'].str.contains(phrase, case=False, na=False).sum()
    phrase_counts[phrase] = count

df_slang = pd.DataFrame(phrase_counts.items(), columns=['term', 'frequency'])
df_slang = df_slang[df_slang['frequency'] > 0].sort_values(by='frequency', ascending=False).reset_index(drop=True)

"""##Look Up Terms In Urban Dictionary"""

df_slang['urban_definition'] = df_slang['term'].apply(get_urban_definitions)

def format_definitions(defs):
    if isinstance(defs, list):
        return '‚Äî ' + '\n‚Äî '.join(defs)
    return None

df_slang['definition_display'] = df_slang['urban_definition'].apply(format_definitions)

df_slang['urban_definition'].apply(type).value_counts()

df_slang

def clean_text(text):
    return text.replace('\r', '').replace('\n', ' ').strip()

df_slang['urban_definition'] = df_slang['urban_definition'].apply(
    lambda defs: [clean_text(d) for d in defs] if isinstance(defs, list) else None
)

#Test it on 1 word
get_urban_definitions("rizz")

#Test it on 1 word
get_urban_definitions("sus")

df_slang.to_csv('slang_terms.csv', index=False)

"""##Visualizations

###Top Slang by Frequency
"""

#df = pd.read_csv("https://github.com/jtmtran/reddit_trending_realtime/raw/refs/heads/main/slang_terms.csv")

import matplotlib.pyplot as plt

top_n = 15
plt.figure(figsize=(10, 6))
plt.barh(df_slang['term'].head(top_n)[::-1], df_slang['frequency'].head(top_n)[::-1])
plt.title('Top Slang Terms from Reddit')
plt.xlabel('Frequency')
plt.ylabel('Slang Term')
plt.tight_layout()
plt.show()

"""###Word Cloud"""

from wordcloud import WordCloud

text = ' '.join(df_slang['term'].values)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(12, 6))
#plt.imshow(wordcloud, interpolation='bilinear')
plt.imshow(wordcloud.to_array(), interpolation='bilinear')
plt.axis('off')
plt.title('Slang Word Cloud')
plt.show()

#pip install streamlit

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# REMOVE this:
##Clean Text + Extract Trending Words
###Clean Text
##Get Top Trending Words
##Look Up Terms In Urban Dictionary
##Visualizations
###Top Slang by Frequency
###Word Cloud

# Load default dataset
df = pd.read_csv("https://raw.githubusercontent.com/jtmtran/reddit_trending_realtime/main/slang_terms.csv")

st.markdown(
    "<a href='https://github.com/jtmtran/reddit_trending_realtime' target='_blank'>"
    "<img style='position: absolute; top: 0; right: 0; border: 0;' "
    "src='https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png?resize=149%2C149' "
    "alt='Fork me on GitHub'>"
    "</a>",
    unsafe_allow_html=True
)

#Upload your own CSV
df_upload = st.file_uploader("Upload a new slang CSV file", type="csv")
if df_upload:
    df = pd.read_csv(df_upload)
    st.success("Uploaded new dataset!")


# Filter settings
st.sidebar.title("‚öôÔ∏è Controls")
st.sidebar.markdown("Filter how many terms to show or upload your own slang CSV.")
min_freq = st.sidebar.slider("Minimum frequency", 1, int(df['frequency'].max()), 5)
top_n = st.sidebar.slider("Top N slang terms to display", 5, 50, 15)
st.sidebar.caption("Adjust the sliders to explore more or less frequent slang.")

# Apply filtering AFTER upload
filtered_df = df[df['frequency'] >= min_freq].sort_values(by='frequency', ascending=False).head(top_n)

# Display title
st.title("üß† TrendLens: Real-Time Slang Radar")
st.markdown(
    "üëã Welcome to **TrendLens**, a live dashboard that scrapes Reddit posts to track trending slang.  \n"
    "üó£ Built for Gen Z, linguists, and trend-watchers.  \n"
    "üîç Explore, define, and visualize how slang moves through the internet ‚Äî in real time."
)

# Bar chart
st.subheader("üìä Most Popular Slang Terms")
st.markdown("**üìà Summary:** This dashboard shows the top trending slang terms scraped from Reddit. Use the controls to explore frequency and meaning in real time.")
fig, ax = plt.subplots(figsize=(10, 6))
ax.barh(filtered_df['term'][::-1], filtered_df['frequency'][::-1])
ax.set_xlabel("Frequency")
ax.set_ylabel("Term")
st.pyplot(fig)

# Word Cloud
st.subheader("‚òÅÔ∏è Visual Word Cloud of Slang")
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(filtered_df['term']))
st.image(wordcloud.to_array(), use_column_width=True)
fig_wc, ax_wc = plt.subplots(figsize=(12, 6))
ax_wc.imshow(wordcloud.to_array(), interpolation='bilinear')
ax_wc.axis('off')
st.pyplot(fig_wc)

# Definitions
st.subheader("üìñ Slang Definitions + Urban Dictionary")
selected_term = st.selectbox("Select a slang term to see its meaning:", filtered_df['term'])
def_row = df[df['term'] == selected_term]

if not def_row.empty:
  try:
    definition = def_row['definition_display'].values[0]
    if isinstance(definition, str) and definition.strip().lower() != "nan" and definition.strip():
        defs = definition.split('\n')
        for i, d in enumerate(defs, 1):
            st.markdown(f"**Definition {i}:**")
            st.info(d.strip())
        st.markdown(f"[üîó View on Urban Dictionary](https://www.urbandictionary.com/define.php?term={selected_term})")
    else:
        st.warning("No definition available for this term.")

  except Exception as e:
    st.error(f"Oops ‚Äî something went wrong loading the definition: {e}")

st.markdown("---")
st.caption("Built with ‚ù§Ô∏è by Jennie Tran | [GitHub](https://github.com/jtmtran)")

#pip install streamlit wordcloud matplotlib